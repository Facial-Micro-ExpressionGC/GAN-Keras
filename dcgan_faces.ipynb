{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_faces.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJ69ALfSdiDv",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive', force_remount=True)\n",
        "  COLAB = True\n",
        "  print(f\"Running from google colab\")\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  COLAB = False\n",
        "  print(f\"Not running from google colab\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZgdfBPBFNR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "4fb65c58-62f1-4277-ed5e-a55a95e72f10"
      },
      "source": [
        "#downloading data from Kaggle\n",
        "\n",
        "!kaggle datasets download -d gasgallo/faces-data-new"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading faces-data-new.zip to /content\n",
            " 97% 87.0M/89.9M [00:01<00:00, 63.6MB/s]\n",
            "100% 89.9M/89.9M [00:01<00:00, 88.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8shQB8HE87_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/gdrive/My\\ Drive/kaggle/kaggle.json /root/.kaggle/.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFCncOKFrhY2",
        "colab_type": "code",
        "outputId": "0063c44e-800e-43f1-e83d-4a2f412c5df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#downloading data from Kaggle\n",
        "!kaggle datasets download -d gasgallo/faces-data-new"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "faces-data-new.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DRGOlf4Fapa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -d faces-data-new -q faces-data-new.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d62FyOfiJ5sw",
        "colab_type": "code",
        "outputId": "0647c0f3-d434-47a3-deba-27bc31a9bb11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.datasets import fashion_mnist, mnist\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class DCGAN_FACES:\n",
        "\n",
        "\tdef __init__(self, rows=28, cols=28, channels=3, generator_resolution=1):\n",
        "\t\n",
        "\t\t# Generation resolution - Must be square \n",
        "\t\t# Training data is also scaled to this.\n",
        "\t\t# Note GENERATE_RES 4 or higher  will blow Google CoLab's memory and have not\n",
        "\t\t# been tested extensivly.\n",
        "\t\tself.GENERATE_RES = generator_resolution # Generation resolution factor (1=32, 2=64, 3=96, 4=128, etc.)\n",
        "\t\tself.GENERATE_SQUARE = rows * self.GENERATE_RES # rows/cols (should be square)\n",
        "\t\tself.IMAGE_CHANNELS = channels\n",
        "\t\t\n",
        "\t\t# Preview image \n",
        "\t\tself.PREVIEW_ROWS = 4\n",
        "\t\tself.PREVIEW_COLS = 7\n",
        "\t\tself.PREVIEW_MARGIN = 16\n",
        "\t\t\n",
        "\t\t# Size vector to generate images from\n",
        "\t\tself.latent_dim = 100\n",
        "\t\t\n",
        "\t\t# Configuration\n",
        "\t\t#DATA_PATH = '/content/drive/My Drive/projects/faces'\n",
        "\t\t#self.DATA_PATH = '/content/gdrive/My Drive/MLProjects/GAN/kaggle-faces-keras'\n",
        "\t\tself.DATA_PATH = 'faces-data-new'\n",
        "\t\t#self.EPOCHS = 50\n",
        "\t\t#BATCH_SIZE = 32\n",
        "\t\t#BUFFER_SIZE = 60000\n",
        "\t\t\n",
        "\t\tprint(f\"Will generate {self.GENERATE_SQUARE}px square images.\")\n",
        "\t\n",
        "\t\tself.rows = rows\n",
        "\t\tself.cols = cols\n",
        "\t\tself.channels = channels\n",
        "\t\tself.image_shape = (self.rows*self.GENERATE_RES, self.cols*self.GENERATE_RES, self.channels)\n",
        "\t\tself.latent_dim = 100\n",
        "\t\tself.sample_rows = 5\n",
        "\t\tself.sample_cols = 5\n",
        "\t\tself.sample_path = 'images'\n",
        "\n",
        "\t\toptimizer = RMSprop(lr=0.0008, clipvalue=1.0, decay=6e-8)\n",
        "\t\t\n",
        "\t\tgenerator_optimizer = Adam(1.5e-4,0.5)\n",
        "\t\tdiscriminator_optimizer = Adam(1.5e-4,0.5)\n",
        "\t\t\n",
        "\t\t# This method returns a helper function to compute cross entropy loss\n",
        "\t\tcross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\t\t\n",
        "\t\t#Get the discriminator and generator Models\n",
        "\t\tprint(\"Build Discriminator\")\n",
        "\t\tself.discriminator = self.build_discriminator()\n",
        "\t\t\n",
        "\t\tself.discriminator.compile(loss='binary_crossentropy', optimizer=discriminator_optimizer, metrics=['accuracy'])\n",
        "  \n",
        "\t\tprint(\"Build Generator\")\n",
        "\t\t\n",
        "\t\tself.generator = self.build_generator()\n",
        "\t\t\n",
        "\t\trandom_input = Input(shape=(self.latent_dim,))\n",
        "\t\tgenerated_image = self.generator(random_input)\n",
        "\t\t\n",
        "\t\tself.discriminator.trainable = False\n",
        "\t\tvalidity = self.discriminator(generated_image)\n",
        "\t\t\n",
        "\t\tprint(\"Build Combined Model\")\n",
        "\t\tself.combined_model = Model(random_input, validity)\n",
        "\t\tself.combined_model.compile(loss='binary_crossentropy', optimizer=generator_optimizer,metrics=['accuracy'])\n",
        "\n",
        "\t# Nicely formatted time string\n",
        "\tdef hms_string(self,sec_elapsed):\n",
        "\t\th = int(sec_elapsed / (60 * 60))\n",
        "\t\tm = int((sec_elapsed % (60 * 60)) / 60)\n",
        "\t\ts = sec_elapsed % 60\n",
        "\t\treturn \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
        "\n",
        "\n",
        "\tdef build_generator(self): #DCGAN - Faces\n",
        "\t\tmodel = Sequential()\n",
        "\t\t\n",
        "\t\tprint(\"Before Dense\")\n",
        "\t\tmodel.add(Dense(7*7*256,activation=\"relu\",input_dim=self.latent_dim, name='Gen_dense_1'))\n",
        "\t\tmodel.add(Reshape((7,7,256)))\n",
        "\t\tmodel.add(UpSampling2D())\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\t\n",
        "\t\tmodel.add(UpSampling2D())\n",
        "\t\tmodel.add(Conv2D(64,kernel_size=3,padding=\"same\"))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\t\n",
        "\t\t# Output resolution, additional upsamplingDATA_PATH'\n",
        "\t\t#model.add(UpSampling2D())\n",
        "\t\t#model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "\t\t#model.add(BatchNormalization(momentum=0.8))\n",
        "\t\t#model.add(Activation(\"relu\"))\n",
        "\t\t\n",
        "\t\tif self.GENERATE_RES>1:\n",
        "\t\t\tmodel.add(UpSampling2D(size=(self.GENERATE_RES,self.GENERATE_RES)))\n",
        "\t\t\tmodel.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n",
        "\t\t\tmodel.add(BatchNormalization(momentum=0.8))\n",
        "\t\t\tmodel.add(Activation(\"relu\"))\n",
        "\t\t\n",
        "\t\t# Final CNN layer\n",
        "\t\tmodel.add(Conv2D(self.channels,kernel_size=3,padding=\"same\"))\n",
        "\t\t#model.add(Activation(\"tanh\"))\n",
        "\t\tmodel.add(Activation(\"sigmoid\"))\n",
        "\t\n",
        "\t\tmodel.summary()\n",
        "\t\n",
        "\t\treturn model\n",
        "\n",
        "\n",
        "\tdef build_discriminator(self): #DCGAN - Faces\n",
        "\t\tmodel = Sequential()\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.image_shape, padding=\"same\"))\n",
        "\t\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
        "\t\tmodel.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "\t\t#model.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\tmodel.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
        "\t\t#model.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\tmodel.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
        "\t\t#model.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\tmodel.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
        "\t\t#model.add(BatchNormalization(momentum=0.8))\n",
        "\t\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t\tmodel.add(Dropout(0.25))\n",
        "\n",
        "\t\tmodel.add(Flatten())\n",
        "\t\tmodel.add(Dense(1, activation='sigmoid',name = 'Des_Dense_1'))\n",
        "\t\n",
        "\t\tmodel.summary()\n",
        "\t\t\n",
        "\t\treturn model\n",
        "\n",
        "\tdef build_discriminator1(self): #GAN\n",
        "\t\n",
        "\t\tdef conv_block(x,filter_size,strides=2, padding='same' ):\n",
        "\t\t\tx = Conv2D(filter_size, (3,3), strides = strides, padding = padding)(x)\n",
        "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
        "\t\t\tx = Dropout(0.3)(x)\n",
        "\t\t\treturn x\n",
        "\t\t\n",
        "\t\tinput_shape = (self.rows, self.cols, self.channels)\n",
        "\t\t\n",
        "\t\tinput_image = Input(shape=input_shape)\n",
        "\t\t\n",
        "\t\tx = conv_block(input_image, 64, strides=(2,2))\n",
        "\t\tx = conv_block(x, 128, strides=(1,1))\n",
        "\t\tx = conv_block(x,256, strides=(2,2))\n",
        "\t\tx = conv_block(x,512, (1,1))\n",
        "\t\t\n",
        "\t\tfeatures = Flatten()(x)\n",
        "\t\treal_or_fake = Dense(1, activation='sigmoid')(features)\n",
        "\t\t\n",
        "\t\t\n",
        "\t\treturn Model(input_image,real_or_fake)\n",
        "\n",
        "\tdef build_generator1(self): #GAN\n",
        "\t\n",
        "\t\n",
        "\t\t\n",
        "\t\tmodel = Sequential()\n",
        "\t\tmodel.add(Dense(7*7*256, input_dim=self.latent_dim))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.9))\n",
        "\t\tmodel.add(Activation('relu'))\n",
        "\t\t\n",
        "\t\tmodel.add(Reshape((7,7,256)))\n",
        "\t\tmodel.add(Dropout(0.4))\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2DTranspose(128,(5,5),padding='same'))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.9))\n",
        "\t\tmodel.add(Activation('relu'))\n",
        "\t\tmodel.add(UpSampling2D())\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2DTranspose(64,(3,3),padding='same'))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.9))\n",
        "\t\tmodel.add(Activation('relu'))\n",
        "\t\tmodel.add(UpSampling2D())\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2DTranspose(32,(3,3),padding='same'))\n",
        "\t\tmodel.add(BatchNormalization(momentum=0.9))\n",
        "\t\tmodel.add(Activation('relu'))\n",
        "\t\t\n",
        "\t\tmodel.add(Conv2DTranspose(1,(3,3),padding='same'))\n",
        "\t\tmodel.add(Activation('sigmoid'))\n",
        "\t\t\n",
        "\t\tinput = Input(shape=(self.latent_dim,))\n",
        "\t\t\n",
        "\t\tgenerated_image = model(input)\n",
        "\t\t\n",
        "\t\t#model.summary()\n",
        "\t\t\n",
        "\t\treturn(Model(input,generated_image))\n",
        "\t\t\n",
        "\t\t\n",
        "\tdef save_images(self,cnt,noise):\n",
        "\t\timage_array = np.full(( \n",
        "\t\t\tself.PREVIEW_MARGIN + (self.PREVIEW_ROWS * (self.GENERATE_SQUARE+self.PREVIEW_MARGIN)), \n",
        "\t\t\tself.PREVIEW_MARGIN + (self.PREVIEW_COLS * (self.GENERATE_SQUARE+self.PREVIEW_MARGIN)), 3), \n",
        "\t\t\t255, dtype=np.uint8)\n",
        "\t\t\n",
        "\t\tgenerated_images = self.generator.predict(noise)\n",
        "\t\t\n",
        "\t\tgenerated_images = 0.5 * generated_images + 0.5\n",
        "\t\t\n",
        "\t\timage_count = 0\n",
        "\t\tfor row in range(self.PREVIEW_ROWS):\n",
        "\t\t\tfor col in range(self.PREVIEW_COLS):\n",
        "\t\t\t\tr = row * (self.GENERATE_SQUARE+16) + self.PREVIEW_MARGIN\n",
        "\t\t\t\tc = col * (self.GENERATE_SQUARE+16) + self.PREVIEW_MARGIN\n",
        "\t\t\t\timage_array[r:r+self.GENERATE_SQUARE,c:c+self.GENERATE_SQUARE] = generated_images[image_count] * 255\n",
        "\t\t\t\timage_count += 1\n",
        "\n",
        "          \n",
        "\t\toutput_path = os.path.join(self.DATA_PATH,'output')\n",
        "\t\tif not os.path.exists(output_path):\n",
        "\t\t  os.makedirs(output_path)\n",
        "  \n",
        "\t\tfilename = os.path.join(output_path,\"train-{:06d}.png\".format(cnt))\n",
        "\t\tim = Image.fromarray(image_array)\n",
        "\t\tim.save(filename)\n",
        "\n",
        "\n",
        "\tdef plot_sample_images(self, epoch, noise):\n",
        "\t\tr, c = self.sample_rows, self.sample_cols\n",
        "\t\t\n",
        "\t\tgen_imgs = self.generator.predict(noise)\n",
        "\t\t\n",
        "\t\tfilename = os.path.join(self.sample_path,'%d.png'% epoch)\n",
        "\t\tfig, axs = plt.subplots(r, c)\n",
        "\t\tcnt = 0\n",
        "\t\tfor i in range(r):\n",
        "\t\t\tfor j in range(c):\n",
        "\t\t\t\taxs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
        "\t\t\t\taxs[i,j].axis('off')\n",
        "\t\t\t\tcnt += 1\n",
        "\t\tfig.savefig(filename)\n",
        "\t\tplt.close()\n",
        "\t\n",
        "\n",
        "\tdef load_faces(self):\n",
        "\t\t# Image set has 11,682 images.  Can take over an hour for initial preprocessing.\n",
        "\t\t# Because of this time needed, save a Numpy preprocessed file.\n",
        "\t\t# Note, that file is large enough to cause problems for sume verisons of Pickle,\n",
        "\t\t# so Numpy binary files are used.\n",
        "\t\ttraining_binary_path = os.path.join(self.DATA_PATH,f'training_data_{self.GENERATE_SQUARE}_{self.GENERATE_SQUARE}.npy')\n",
        "\n",
        "\t\tprint(f\"Looking for file: {training_binary_path}\")\n",
        "\n",
        "\t\tstart = time.time()\n",
        "\n",
        "\t\tif not os.path.isfile(training_binary_path):\n",
        "\t\t\t#start = time.time()\n",
        "\t\t\tprint(\"Loading training images...\")\n",
        "\n",
        "\t\t\ttraining_data = []\n",
        "\t\t\t#faces_path = os.path.join(DATA_PATH,'face_images')\n",
        "\t\t\tfaces_path = os.path.join(self.DATA_PATH,'images')\n",
        "\t\t\tfor filename in tqdm(os.listdir(faces_path)):\n",
        "\t\t\t\tpath = os.path.join(faces_path,filename)\n",
        "\t\t\t\tif os.path.isfile(path):\n",
        "\t\t\t\t\timage = Image.open(path).resize((self.GENERATE_SQUARE,self.GENERATE_SQUARE),Image.ANTIALIAS)\n",
        "\t\t\t\t\ttraining_data.append(np.asarray(image))\n",
        "\t\t\ttraining_data = np.reshape(training_data,(-1,self.GENERATE_SQUARE,self.GENERATE_SQUARE,self.IMAGE_CHANNELS))\n",
        "\t\t\ttraining_data = training_data.astype(np.float32)\n",
        "\t\t\t#training_data = training_data / 127.5 - 1.\n",
        "\t\t\t#training_data = training_data / 255.\n",
        "\t\t\tprint(f'training_data.shape: {training_data.shape}')\n",
        "\n",
        "\n",
        "\t\t\tprint(\"Saving training image binary...\")\n",
        "\t\t\tnp.save(training_binary_path,training_data)\n",
        "\t\t\telapsed = time.time()-start\n",
        "\t\t\tprint (f'Image preprocess time: {self.hms_string(elapsed)}')\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Loading previous training numpy...\")\n",
        "\t\t\ttraining_data = np.load(training_binary_path)\n",
        "\n",
        "\t\treturn training_data\n",
        "\n",
        "\tdef plot_loss(self,losses):\n",
        "\t\t\"\"\"\n",
        "\t\t@losses.keys():\n",
        "\t\t\t0: loss\n",
        "\t\t\t1: accuracy\n",
        "\t\t\"\"\"\n",
        "\t\td_loss = [v[0] for v in losses[\"D\"]]\n",
        "\t\tg_loss = [v[0] for v in losses[\"G\"]]\n",
        "\t\t\n",
        "\t\tplt.figure(figsize=(10,8))\n",
        "\t\tplt.plot(d_loss, label=\"Discriminator loss\")\n",
        "\t\tplt.plot(g_loss, label=\"Generator loss\")\n",
        "\t\t\n",
        "\t\tplt.xlabel('Epochs')\n",
        "\t\tplt.ylabel('Loss')\n",
        "\t\tplt.legend()\n",
        "\t\tplt.show()\n",
        "\n",
        "\n",
        "\t\t\n",
        "\tdef train(self, epochs=10000, batch_size=32, save_freq=200):\n",
        "\t\t\n",
        "\t\t#Load Dataset\n",
        "\t\t#(x_train,_),(_,_) = mnist.load_data()\n",
        "\t\t\n",
        "\t\tx_train = self.load_faces()\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t#(x_train,_),(_,_) = fashion_mnist.load_data()\n",
        "\t\t\n",
        "\t\t#normalize and reset train set in range (0,1) # normalizing to (-1,1) seems to be not working.\n",
        "\t\t\n",
        "\t\t#x_train = np.expand_dims(x_train, axis=-1)\n",
        "\t\t\n",
        "\t\t#x_train = (x_train.astype('float32') / 127.5 ) - 1. # Normalizing this way doesn't work during training.\n",
        "\t\t#x_train = (x_train / 127.5 ) - 1.\n",
        "\t\t\n",
        "\t\tx_train = x_train.astype('float32')/255.0 #Normalizing  this way does work during training.\n",
        "\t\t\n",
        "\t\t\n",
        "\t\tprint(\"x_train.shape\",x_train.shape)\n",
        "\t\n",
        "\t\tplt.imshow(x_train[0])\n",
        "\t\t\n",
        "\t\t#Ground Truth. Setting real images labels to True\n",
        "\t\ty_real = np.ones((batch_size,1))\n",
        "\t\t\n",
        "\t\t#Setting fake images labels to False\n",
        "\t\ty_fake = np.zeros((batch_size,1))\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t#fixed_seed = np.random.normal(0,1,size=[25,self.latent_dim])\n",
        "\t\t\n",
        "\t\tcnt = 1\n",
        "\t\t\n",
        "\t\t#Generating Fixed noise to be passed for sampling with same inputs after set of epochs and seeing the results\n",
        "\t\t#noise_input = np.random.normal(0,1,size=[self.sample_rows*self.sample_cols,self.latent_dim])\n",
        "\t\tfixed_seed = np.random.normal(0, 1, (self.PREVIEW_ROWS * self.PREVIEW_COLS, self.latent_dim))\n",
        "\t\tstart = time.time()\n",
        "\t\t\n",
        "\t\t#Setup loss vector to store losses for Generator and Discriminator\n",
        "\t\t\n",
        "\t\tlosses = {\"D\":[], \"G\":[]}\n",
        "\t\t\n",
        "\t\tpath = self.sample_path\n",
        "\t\tif not os.path.isdir(path):\n",
        "\t\t  os.mkdir(path)\n",
        "\t\t\n",
        "\t\tfor epoch in range(epochs):\n",
        "\t\t\n",
        "\t\t\tepoch_start = time.time()\n",
        "\t\t\t#Training of Discriminator. Taking random samples of batch_size #\n",
        "\t\t\tnoise = np.random.normal(0,1,size=[batch_size,self.latent_dim])\n",
        "\t\t\t\n",
        "\t\t\t#take random batched of indexes for x_train\n",
        "\t\t\tidx = np.random.randint(0,x_train.shape[0],size=batch_size)\n",
        "\t\t\t\n",
        "\t\t\t#print(idx[0:10])\n",
        "\t\t\tx_real = x_train[idx]\n",
        "\t\t\t\n",
        "\t\t\t#print(\"x_real.shape\",x_real.shape)\n",
        "\t\t\t\n",
        "\t\t\t#Generate some fake images\n",
        "\t\t\tx_fake = self.generator.predict(noise)\n",
        "\t\t\t\t\n",
        "\t\t\tx = np.concatenate((x_real,x_fake))\n",
        "\t\t\t\n",
        "\t\t\ty = np.ones([2*batch_size,1])\n",
        "\t\t\ty[batch_size:,:] = 0\n",
        "\t\t\t\n",
        "\t\t\t#Train discriminator on real and fake\n",
        "\t\t\td_loss = self.discriminator.train_on_batch(x,y)\n",
        "\t\t\t\n",
        "\t\t\t#Train Generator on Calculated loss\n",
        "\t\t\ty = np.ones([batch_size, 1])\n",
        "\t\t\t\t\n",
        "\t\t\t#noise = np.random.uniform(-1.0, 1.0, size=[batch_size, self.latent_dim])\n",
        "\t\t\tnoise = np.random.normal(0,1,size=[batch_size,self.latent_dim])\n",
        "\t\t\t\n",
        "\t\t\tg_loss = self.combined_model.train_on_batch(noise,y)\n",
        "\t\t\t\n",
        "\t\t\tlosses[\"D\"].append(d_loss)\n",
        "\t\t\tlosses[\"G\"].append(g_loss)\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\t\n",
        "\t\t\t#Time for an update\n",
        "\t\t\t\n",
        "\t\t\tif save_freq > 0:\n",
        "\t\t\t\tif epoch % save_freq == 0:\n",
        "\t\t\t\t\t\n",
        "\t\t\t\t\tepoch_elapsed = time.time()-epoch_start\n",
        "\t\t\t\t\t#print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {self.hms_string(epoch_elapsed)}')\n",
        "\t\t\t\t\tprint (\"epoch %d: [D loss: %f, acc.: %.2f%%] [G loss: %f, acc.: %.2f%%] elapsed time [%s]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], 100*g_loss[1], self.hms_string(epoch_elapsed)))\n",
        "\t\t\t\t\n",
        "\t\t\t\t\t#self.plot_sample_images(epoch, fixed_seed)\n",
        "\t\t\t\t\tself.save_images(epoch,fixed_seed)\n",
        "\t\t\t\t\tcnt+=1\n",
        "\t\telapsed = time.time()-start\n",
        "\t\tprint (f'Training time: {self.hms_string(elapsed)}')\n",
        "\t\t\t\t\t\n",
        "\t\tself.plot_loss(losses)\n",
        "\t\t\n",
        "if __name__ == '__main__':\n",
        "\tgenerator_resolution = 3\n",
        "\trows=28\n",
        "\tcols=28\n",
        "\tchannels=3\n",
        "\t\n",
        "\tgan = DCGAN_FACES(rows=rows, cols=cols, channels=channels,generator_resolution=generator_resolution)\n",
        "\t#gan.train(epochs=14000, batch_size=32, save_freq=200)\t\t\n",
        "\tgan.train(epochs=50000,batch_size=32,save_freq=100)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Will generate 84px square images.\n",
            "Build Discriminator\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 42, 42, 32)        896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      (None, 42, 42, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 42, 42, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 21, 21, 64)        18496     \n",
            "_________________________________________________________________\n",
            "zero_padding2d (ZeroPadding2 (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 128)       73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 256)       295168    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 11, 11, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 11, 11, 512)       0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 11, 11, 512)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 61952)             0         \n",
            "_________________________________________________________________\n",
            "Des_Dense_1 (Dense)          (None, 1)                 61953     \n",
            "=================================================================\n",
            "Total params: 1,630,529\n",
            "Trainable params: 1,630,529\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Build Generator\n",
            "Before Dense\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Gen_dense_1 (Dense)          (None, 12544)             1266944   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 7, 7, 256)         0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d (UpSampling2D) (None, 14, 14, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 14, 14, 128)       295040    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 14, 14, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2 (None, 28, 28, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 64)        73792     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2 (None, 84, 84, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 84, 84, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 84, 84, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 84, 84, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 84, 84, 3)         3459      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 84, 84, 3)         0         \n",
            "=================================================================\n",
            "Total params: 1,714,371\n",
            "Trainable params: 1,713,731\n",
            "Non-trainable params: 640\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 50/7865 [00:00<00:15, 493.05it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Build Combined Model\n",
            "Looking for file: faces-data-new/training_data_84_84.npy\n",
            "Loading training images...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7865/7865 [00:12<00:00, 654.07it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training_data.shape: (7864, 84, 84, 3)\n",
            "Saving training image binary...\n",
            "Image preprocess time: 0:00:14.12\n",
            "x_train.shape (7864, 84, 84, 3)\n",
            "epoch 0: [D loss: 0.696271, acc.: 37.50%] [G loss: 0.795267, acc.: 0.00%] elapsed time [0:00:08.42]\n",
            "epoch 100: [D loss: 0.544215, acc.: 71.88%] [G loss: 0.518802, acc.: 90.62%] elapsed time [0:00:00.07]\n",
            "epoch 200: [D loss: 0.287138, acc.: 96.88%] [G loss: 0.325840, acc.: 100.00%] elapsed time [0:00:00.08]\n",
            "epoch 300: [D loss: 0.433107, acc.: 92.19%] [G loss: 1.244574, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 400: [D loss: 0.610844, acc.: 65.62%] [G loss: 0.973664, acc.: 12.50%] elapsed time [0:00:00.08]\n",
            "epoch 500: [D loss: 0.561335, acc.: 82.81%] [G loss: 0.937841, acc.: 31.25%] elapsed time [0:00:00.08]\n",
            "epoch 600: [D loss: 0.541663, acc.: 75.00%] [G loss: 0.724343, acc.: 56.25%] elapsed time [0:00:00.08]\n",
            "epoch 700: [D loss: 0.676401, acc.: 64.06%] [G loss: 0.521737, acc.: 78.12%] elapsed time [0:00:00.07]\n",
            "epoch 800: [D loss: 0.620806, acc.: 64.06%] [G loss: 1.159753, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 900: [D loss: 0.627746, acc.: 68.75%] [G loss: 0.734969, acc.: 40.62%] elapsed time [0:00:00.08]\n",
            "epoch 1000: [D loss: 0.689472, acc.: 56.25%] [G loss: 0.876594, acc.: 34.38%] elapsed time [0:00:00.08]\n",
            "epoch 1100: [D loss: 0.621234, acc.: 65.62%] [G loss: 0.771226, acc.: 34.38%] elapsed time [0:00:00.08]\n",
            "epoch 1200: [D loss: 0.569804, acc.: 78.12%] [G loss: 0.871652, acc.: 21.88%] elapsed time [0:00:00.07]\n",
            "epoch 1300: [D loss: 0.631928, acc.: 59.38%] [G loss: 0.875378, acc.: 31.25%] elapsed time [0:00:00.07]\n",
            "epoch 1400: [D loss: 0.596841, acc.: 71.88%] [G loss: 0.791076, acc.: 37.50%] elapsed time [0:00:00.07]\n",
            "epoch 1500: [D loss: 0.594901, acc.: 73.44%] [G loss: 0.852463, acc.: 25.00%] elapsed time [0:00:00.08]\n",
            "epoch 1600: [D loss: 0.624360, acc.: 59.38%] [G loss: 0.994233, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 1700: [D loss: 0.666950, acc.: 57.81%] [G loss: 0.973009, acc.: 31.25%] elapsed time [0:00:00.07]\n",
            "epoch 1800: [D loss: 0.592933, acc.: 65.62%] [G loss: 0.686707, acc.: 56.25%] elapsed time [0:00:00.08]\n",
            "epoch 1900: [D loss: 0.593221, acc.: 62.50%] [G loss: 0.946871, acc.: 25.00%] elapsed time [0:00:00.09]\n",
            "epoch 2000: [D loss: 0.629625, acc.: 60.94%] [G loss: 1.169924, acc.: 12.50%] elapsed time [0:00:00.08]\n",
            "epoch 2100: [D loss: 0.541033, acc.: 75.00%] [G loss: 1.283890, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 2200: [D loss: 0.494955, acc.: 79.69%] [G loss: 1.148733, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 2300: [D loss: 0.569056, acc.: 75.00%] [G loss: 1.154235, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 2400: [D loss: 0.601348, acc.: 62.50%] [G loss: 0.764936, acc.: 40.62%] elapsed time [0:00:00.08]\n",
            "epoch 2500: [D loss: 0.509955, acc.: 76.56%] [G loss: 1.167965, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 2600: [D loss: 0.512049, acc.: 76.56%] [G loss: 0.718194, acc.: 46.88%] elapsed time [0:00:00.07]\n",
            "epoch 2700: [D loss: 0.469253, acc.: 78.12%] [G loss: 1.265096, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 2800: [D loss: 0.470090, acc.: 81.25%] [G loss: 1.132737, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 2900: [D loss: 0.445284, acc.: 85.94%] [G loss: 1.430402, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 3000: [D loss: 0.492855, acc.: 71.88%] [G loss: 1.300776, acc.: 21.88%] elapsed time [0:00:00.08]\n",
            "epoch 3100: [D loss: 0.392064, acc.: 90.62%] [G loss: 1.135990, acc.: 25.00%] elapsed time [0:00:00.08]\n",
            "epoch 3200: [D loss: 0.454395, acc.: 84.38%] [G loss: 1.451401, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 3300: [D loss: 0.492154, acc.: 71.88%] [G loss: 1.485055, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 3400: [D loss: 0.504844, acc.: 70.31%] [G loss: 1.250140, acc.: 21.88%] elapsed time [0:00:00.07]\n",
            "epoch 3500: [D loss: 0.559434, acc.: 68.75%] [G loss: 1.229630, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 3600: [D loss: 0.487372, acc.: 71.88%] [G loss: 1.254983, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 3700: [D loss: 0.395405, acc.: 81.25%] [G loss: 1.828325, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 3800: [D loss: 0.594275, acc.: 70.31%] [G loss: 1.676096, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 3900: [D loss: 0.426447, acc.: 78.12%] [G loss: 1.260644, acc.: 21.88%] elapsed time [0:00:00.09]\n",
            "epoch 4000: [D loss: 0.601158, acc.: 64.06%] [G loss: 1.374962, acc.: 15.62%] elapsed time [0:00:00.08]\n",
            "epoch 4100: [D loss: 0.452608, acc.: 79.69%] [G loss: 1.323100, acc.: 21.88%] elapsed time [0:00:00.08]\n",
            "epoch 4200: [D loss: 0.450698, acc.: 79.69%] [G loss: 1.579356, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 4300: [D loss: 0.367170, acc.: 85.94%] [G loss: 1.284760, acc.: 15.62%] elapsed time [0:00:00.07]\n",
            "epoch 4400: [D loss: 0.471512, acc.: 73.44%] [G loss: 1.209903, acc.: 12.50%] elapsed time [0:00:00.08]\n",
            "epoch 4500: [D loss: 0.429247, acc.: 79.69%] [G loss: 1.387908, acc.: 15.62%] elapsed time [0:00:00.08]\n",
            "epoch 4600: [D loss: 0.439633, acc.: 81.25%] [G loss: 1.578534, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 4700: [D loss: 0.474757, acc.: 73.44%] [G loss: 1.529452, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 4800: [D loss: 0.465524, acc.: 73.44%] [G loss: 1.353761, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 4900: [D loss: 0.369238, acc.: 76.56%] [G loss: 1.361054, acc.: 15.62%] elapsed time [0:00:00.07]\n",
            "epoch 5000: [D loss: 0.356064, acc.: 87.50%] [G loss: 1.324051, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 5100: [D loss: 0.363240, acc.: 84.38%] [G loss: 1.414403, acc.: 15.62%] elapsed time [0:00:00.08]\n",
            "epoch 5200: [D loss: 0.567799, acc.: 67.19%] [G loss: 2.141104, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 5300: [D loss: 0.377500, acc.: 87.50%] [G loss: 2.326151, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 5400: [D loss: 0.412323, acc.: 84.38%] [G loss: 1.773016, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 5500: [D loss: 0.389804, acc.: 84.38%] [G loss: 1.473820, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 5600: [D loss: 0.399973, acc.: 84.38%] [G loss: 1.541395, acc.: 15.62%] elapsed time [0:00:00.08]\n",
            "epoch 5700: [D loss: 0.445254, acc.: 75.00%] [G loss: 1.689930, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 5800: [D loss: 0.498320, acc.: 70.31%] [G loss: 1.833046, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 5900: [D loss: 0.532258, acc.: 70.31%] [G loss: 1.106198, acc.: 28.12%] elapsed time [0:00:00.08]\n",
            "epoch 6000: [D loss: 0.473523, acc.: 75.00%] [G loss: 2.040453, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 6100: [D loss: 0.271205, acc.: 90.62%] [G loss: 2.496931, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 6200: [D loss: 0.372385, acc.: 82.81%] [G loss: 1.780367, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 6300: [D loss: 0.327585, acc.: 82.81%] [G loss: 1.765654, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 6400: [D loss: 0.367027, acc.: 85.94%] [G loss: 1.220445, acc.: 25.00%] elapsed time [0:00:00.07]\n",
            "epoch 6500: [D loss: 0.292174, acc.: 90.62%] [G loss: 1.957548, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 6600: [D loss: 0.295357, acc.: 90.62%] [G loss: 2.411086, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 6700: [D loss: 0.376164, acc.: 85.94%] [G loss: 1.405546, acc.: 18.75%] elapsed time [0:00:00.07]\n",
            "epoch 6800: [D loss: 0.421951, acc.: 79.69%] [G loss: 1.444248, acc.: 28.12%] elapsed time [0:00:00.08]\n",
            "epoch 6900: [D loss: 0.361617, acc.: 81.25%] [G loss: 1.792959, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 7000: [D loss: 0.255259, acc.: 89.06%] [G loss: 2.032939, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 7100: [D loss: 0.377869, acc.: 78.12%] [G loss: 2.286129, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 7200: [D loss: 0.352203, acc.: 87.50%] [G loss: 1.504619, acc.: 15.62%] elapsed time [0:00:00.07]\n",
            "epoch 7300: [D loss: 0.218070, acc.: 95.31%] [G loss: 1.994650, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 7400: [D loss: 0.286011, acc.: 90.62%] [G loss: 2.331200, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 7500: [D loss: 0.287909, acc.: 87.50%] [G loss: 1.594056, acc.: 21.88%] elapsed time [0:00:00.07]\n",
            "epoch 7600: [D loss: 0.382484, acc.: 79.69%] [G loss: 2.338793, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 7700: [D loss: 0.372053, acc.: 85.94%] [G loss: 1.959757, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 7800: [D loss: 0.402441, acc.: 84.38%] [G loss: 1.702879, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 7900: [D loss: 0.274786, acc.: 85.94%] [G loss: 2.135005, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 8000: [D loss: 0.381352, acc.: 82.81%] [G loss: 1.543689, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 8100: [D loss: 0.268274, acc.: 93.75%] [G loss: 2.102563, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 8200: [D loss: 0.366595, acc.: 84.38%] [G loss: 2.375864, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 8300: [D loss: 0.258625, acc.: 95.31%] [G loss: 1.837548, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 8400: [D loss: 0.296178, acc.: 87.50%] [G loss: 2.296978, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 8500: [D loss: 0.284870, acc.: 89.06%] [G loss: 2.134946, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 8600: [D loss: 0.337987, acc.: 87.50%] [G loss: 1.918959, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 8700: [D loss: 0.407192, acc.: 82.81%] [G loss: 1.842505, acc.: 18.75%] elapsed time [0:00:00.07]\n",
            "epoch 8800: [D loss: 0.189334, acc.: 92.19%] [G loss: 2.172916, acc.: 15.62%] elapsed time [0:00:00.07]\n",
            "epoch 8900: [D loss: 0.358753, acc.: 84.38%] [G loss: 1.746156, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 9000: [D loss: 0.342011, acc.: 82.81%] [G loss: 1.941288, acc.: 12.50%] elapsed time [0:00:00.08]\n",
            "epoch 9100: [D loss: 0.353631, acc.: 81.25%] [G loss: 2.046782, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 9200: [D loss: 0.280414, acc.: 89.06%] [G loss: 1.893429, acc.: 9.38%] elapsed time [0:00:00.09]\n",
            "epoch 9300: [D loss: 0.252439, acc.: 90.62%] [G loss: 2.234364, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 9400: [D loss: 0.335910, acc.: 84.38%] [G loss: 2.485631, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 9500: [D loss: 0.309736, acc.: 85.94%] [G loss: 1.754900, acc.: 18.75%] elapsed time [0:00:00.07]\n",
            "epoch 9600: [D loss: 0.271757, acc.: 90.62%] [G loss: 2.675020, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 9700: [D loss: 0.307034, acc.: 82.81%] [G loss: 2.607623, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 9800: [D loss: 0.218493, acc.: 89.06%] [G loss: 1.910345, acc.: 18.75%] elapsed time [0:00:00.07]\n",
            "epoch 9900: [D loss: 0.274949, acc.: 87.50%] [G loss: 2.542459, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 10000: [D loss: 0.233824, acc.: 89.06%] [G loss: 2.446867, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 10100: [D loss: 0.254245, acc.: 89.06%] [G loss: 2.698908, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 10200: [D loss: 0.215429, acc.: 93.75%] [G loss: 2.554413, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 10300: [D loss: 0.309885, acc.: 85.94%] [G loss: 2.728662, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 10400: [D loss: 0.301581, acc.: 89.06%] [G loss: 2.186601, acc.: 12.50%] elapsed time [0:00:00.08]\n",
            "epoch 10500: [D loss: 0.345757, acc.: 82.81%] [G loss: 2.714832, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 10600: [D loss: 0.442484, acc.: 84.38%] [G loss: 2.247120, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 10800: [D loss: 0.121369, acc.: 98.44%] [G loss: 2.029589, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 10900: [D loss: 0.265121, acc.: 89.06%] [G loss: 2.463715, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 11000: [D loss: 0.240457, acc.: 92.19%] [G loss: 2.375344, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 11100: [D loss: 0.192171, acc.: 92.19%] [G loss: 2.078307, acc.: 18.75%] elapsed time [0:00:00.08]\n",
            "epoch 11200: [D loss: 0.279312, acc.: 90.62%] [G loss: 2.495519, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 11300: [D loss: 0.159164, acc.: 96.88%] [G loss: 3.024109, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 11400: [D loss: 0.131068, acc.: 93.75%] [G loss: 3.108490, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 11500: [D loss: 0.200613, acc.: 90.62%] [G loss: 3.428938, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 11600: [D loss: 0.245568, acc.: 84.38%] [G loss: 2.572384, acc.: 18.75%] elapsed time [0:00:00.07]\n",
            "epoch 11700: [D loss: 0.211208, acc.: 93.75%] [G loss: 2.971909, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 11800: [D loss: 0.235712, acc.: 92.19%] [G loss: 2.552498, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 11900: [D loss: 0.282404, acc.: 81.25%] [G loss: 3.439577, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 12000: [D loss: 0.210953, acc.: 95.31%] [G loss: 3.166466, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 12100: [D loss: 0.216355, acc.: 87.50%] [G loss: 2.638815, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 12200: [D loss: 0.299311, acc.: 84.38%] [G loss: 2.769546, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 12300: [D loss: 0.275265, acc.: 89.06%] [G loss: 2.499343, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 12400: [D loss: 0.234383, acc.: 90.62%] [G loss: 2.375113, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 12500: [D loss: 0.292139, acc.: 81.25%] [G loss: 3.576550, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 12600: [D loss: 0.163846, acc.: 96.88%] [G loss: 3.255631, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 12700: [D loss: 0.225807, acc.: 87.50%] [G loss: 2.466285, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 12800: [D loss: 0.282562, acc.: 90.62%] [G loss: 2.917947, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 12900: [D loss: 0.183758, acc.: 95.31%] [G loss: 2.270066, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 13000: [D loss: 0.252283, acc.: 90.62%] [G loss: 3.093543, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 13100: [D loss: 0.113977, acc.: 95.31%] [G loss: 2.667077, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 13200: [D loss: 0.202084, acc.: 92.19%] [G loss: 3.459756, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 13300: [D loss: 0.132678, acc.: 95.31%] [G loss: 1.894596, acc.: 25.00%] elapsed time [0:00:00.08]\n",
            "epoch 13400: [D loss: 0.238921, acc.: 90.62%] [G loss: 2.879235, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 13500: [D loss: 0.261074, acc.: 87.50%] [G loss: 2.208049, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 13600: [D loss: 0.167925, acc.: 95.31%] [G loss: 3.864192, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 13700: [D loss: 0.209306, acc.: 89.06%] [G loss: 2.505053, acc.: 0.00%] elapsed time [0:00:00.07]\n",
            "epoch 13800: [D loss: 0.146344, acc.: 95.31%] [G loss: 4.133857, acc.: 3.12%] elapsed time [0:00:00.08]\n",
            "epoch 13900: [D loss: 0.329114, acc.: 89.06%] [G loss: 3.669046, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 14000: [D loss: 0.234357, acc.: 90.62%] [G loss: 2.438019, acc.: 9.38%] elapsed time [0:00:00.08]\n",
            "epoch 14100: [D loss: 0.076627, acc.: 98.44%] [G loss: 2.862830, acc.: 6.25%] elapsed time [0:00:00.08]\n",
            "epoch 14200: [D loss: 0.195013, acc.: 90.62%] [G loss: 2.274214, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 14300: [D loss: 0.132621, acc.: 93.75%] [G loss: 4.089159, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 14400: [D loss: 0.237930, acc.: 89.06%] [G loss: 3.378465, acc.: 3.12%] elapsed time [0:00:00.09]\n",
            "epoch 14500: [D loss: 0.349554, acc.: 85.94%] [G loss: 2.347251, acc.: 9.38%] elapsed time [0:00:00.07]\n",
            "epoch 14600: [D loss: 0.193368, acc.: 92.19%] [G loss: 3.830344, acc.: 3.12%] elapsed time [0:00:00.07]\n",
            "epoch 14700: [D loss: 0.218470, acc.: 93.75%] [G loss: 4.029409, acc.: 0.00%] elapsed time [0:00:00.08]\n",
            "epoch 14800: [D loss: 0.150340, acc.: 93.75%] [G loss: 2.885832, acc.: 12.50%] elapsed time [0:00:00.07]\n",
            "epoch 14900: [D loss: 0.188572, acc.: 92.19%] [G loss: 3.428952, acc.: 6.25%] elapsed time [0:00:00.07]\n",
            "epoch 15000: [D loss: 0.166814, acc.: 93.75%] [G loss: 3.902323, acc.: 6.25%] elapsed time [0:00:00.09]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0e5f436f1369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN_FACES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerator_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_resolution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0;31m#gan.train(epochs=14000, batch_size=32, save_freq=200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-0e5f436f1369>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_freq)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"D\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1076\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m           standalone=True)\n\u001b[0m\u001b[1;32m   1079\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m   1080\u001b[0m                  outputs['metrics'])\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics, standalone)\u001b[0m\n\u001b[1;32m    431\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTtDRCDjhWsQ",
        "colab_type": "code",
        "outputId": "12f8eaf4-afbe-4c95-c46f-7de46bded248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#!rm -r images/*\n",
        "#!rm -r faces-data-new/output/*\n",
        "#!rm -r gdrive/My\\ Drive/MLProjects/GAN/kaggle-faces-keras/output/*\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'images/*': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRu_FP9DsNLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imageio\n",
        "images = []\n",
        "faces_path  = os.path.join(\"faces-data-new\",\"output\")\n",
        "for filename in sorted(os.listdir(faces_path)):\n",
        "  file = os.path.join(faces_path,filename)\n",
        "  print(filename)\n",
        "  images.append(imageio.imread(file))\n",
        "imageio.mimsave('gan_faces_movie3.gif', images,duration = 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlyYescrMnhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp gan_faces_movie3.gif gdrive/My\\ Drive/MLProjects/."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}